[["index.html", "Code for Quantitative Bias Analysis 1 About", " Code for Quantitative Bias Analysis Jeremy Brown 1 About This site contains code for the applied examples in the article “Quantifying possible bias in clinical and epidemiological studies using quantitative bias analysis” in addition to illustrative example code to conduct alternative quantitative bias analyses. "],["applied-examples.html", "2 Applied examples 2.1 Bias formulas for selection bias 2.2 E-values for unmeasured confounding 2.3 Probabilistic bias analysis for misclassification", " 2 Applied examples In “Applying quantitative bias analysis in clinical and epidemiological studies using quantitative bias analysis” we present three applied examples of bias formulas, bounding methods, and probabilistic bias analysis. Code to generate results for these applied examples is presented below. 2.1 Bias formulas for selection bias In a cohort study of pregnant women investigating the association between maternal lithium use, relative to non-use, and cardiac malformations in live-born infants, a covariate-adjusted risk ratio was estimated of 1.65 (95% CI, 1.02-2.68).(Patorno et al. 2017) Only live-born infants only were included in the study, and as such there was potential for selection bias if there were differences in termination probabilities of foetuses with cardiac malformations by exposure group. Given that the outcome was rare, and therefore odds ratios and risk ratios are approximately equivalent, bias formulas using odds ratios were applied.(Greenland 1996) \\[ OR_{BiasAdjusted} = OR_{Observed}\\frac{S_{01}S_{10}}{S_{00}S_{11}} \\] Values for the bias parameters, selection probabilities by exposure and outcome status, were specified based on the literature. The selection probability of unexposed without cardiac malformations was assumed to be 0.8 (i.e. 20% probability of termination). The selection probability of unexposed with cardiac malformations was varied from 0.5 to 0.7. (30-50% probability of termination). Selection probabilities of exposed were defined by outcome status relative to unexposed (0% to -10% difference). library(tidyverse) library(ggplot2) # Define observed odds ratio observed_rr &lt;- 1.65 # Define bias parameters S00 &lt;- 0.8 S01 &lt;- c(0.5, 0.525, 0.55, 0.575, 0.6, 0.625, 0.65, 0.675, 0.7) differences &lt;- c(0, -0.05, -0.1) # Define function to calculate bias-adjusted risk ratio calc_bias_adj_or &lt;- function(or, s01, s10, s00, s11) { bias_adj_or &lt;- or * (s01*s10)/(s00*s11) return(bias_adj_or) } # Calculate bias-adjusted estimate for different values of bias parameters results &lt;- NULL for (s01 in S01) { for (diff in differences) { bias_adj_rr &lt;- calc_bias_adj_or(observed_rr, s01, S00 + diff, S00, s01 + diff) results_row &lt;- tibble_row(bias_adj_rr=bias_adj_rr, s01=s01, diff=as.character(diff)) results &lt;- bind_rows(results, results_row) } } # Tidy label for difference in selection probabilities results &lt;- results %&gt;% mutate(diff=factor(diff, levels=c(&quot;0&quot;, &quot;-0.05&quot;, &quot;-0.1&quot;), labels=c(&quot;0&quot;, &quot;-5%&quot;, &quot;-10%&quot;))) # Plot figure of bias-adjusted estimates ggplot(data = results, aes(x=s01, y=bias_adj_rr, colour=diff)) + geom_line() + theme_minimal() + ylim(1.4, 2) + scale_x_reverse() + xlab(&quot;Selection probability of unexposed with cardiac malformations&quot;) + ylab(&quot;Bias-adjusted risk ratio&quot;) + guides(colour=guide_legend(title=&quot;Difference in selection\\nprobability of exposed&quot;)) + theme(legend.title=element_text(size=10)) Figure 2.1: Bias-adjusted risk ratio for different assumed selection probabilities The bias-adjusted risk ratios ranged from 1.65 to 1.80 (Figure 2.1), indicating robustness of the point estimate to selection bias under the given assumptions. We can likewise calculate bias-adjusted estimates for the lower bound of the confidence interval. Figure 2.2: Bias-adjusted risk ratio for lower bound of 95% confidence interval for different assumed selection probabilities 2.2 E-values for unmeasured confounding In a cohort study conducted in electronic health records investigating the association between proton pump inhibitors, relative to H2 receptor antagonists, and all-cause mortality, investigators found that individuals were at higher risk of death (covariate-adjusted hazard ratio [HR] 1.38, 95% CI 1.33-1.44).(Brown et al. 2021) However, it was considered that there may be unmeasured confounding due to differences in frailty between individuals prescribed proton pump inhibitors. The prevalence of the unmeasured confounder was not known in either exposure group, and therefore rather than use bias formulas, an E-value was calculated.(Ding and VanderWeele 2016) Given the outcome was rare, the E-value method can be applied to the hazard ratio. \\[ \\text{E-value} = RR_{Obs} + \\sqrt{RR_{Obs}(RR_{Obs} -1)} \\] We can use the EValue package to calculate E-Values. # load EValue package and ggplotify library(EValue) #Calculate E-values evalues.HR(est=1.38, lo=1.33, hi=1.44, rare=TRUE) %&gt;% knitr::kable() point lower upper RR 1.380000 1.330000 1.44 E-values 2.104155 1.992495 NA And we can use bias_plot from the EValue package to display an E-value plot. # Generate E-value plot for point estimate bias_plot(1.38, xmax=9) Figure 2.3: E-value plot for point estimate The E-value for the point estimate was 2.10 and for the lower bound of the point estimate was 1.99. This represents the minimum strength of association that an unmeasured confounder would need to have with either exposure or outcome to reduce the hazard ratio to the null (i.e. 1). An unmeasured confounder with strength of association with exposure and outcome below the line in the plot could not possibly explain, on its own, the observed association. Risk ratios between frailty and mortality &gt;2 have been observed in the literature, and as such we could not rule out unmeasured confounding as a possible explanation for findings based on the E-value. However, as we did not specify prevalence of an unmeasured confounder, we cannot say whether such confounding was likely to account for the observed association. There may also have been additional unmeasured or partially measured confounders contributing to the observed association. 2.3 Probabilistic bias analysis for misclassification In a cohort study of pregnant women conducted using insurance claims data, the observed covariate-adjusted risk ratio for the association between antidepressant use and congenital cardiac defects, was 1.02 (95% CI, 0.90-1.15).(Huybrechts et al. 2014) Some misclassification of the outcome, congenital cardiac malformation was anticipated. Therefore, probabilistic bias analysis was carried out.(Fox, Lash, and Greenland 2005) Code is not provided for this analysis, which was carrier out using SAS and participant record-level data (see sensmac macro for a SAS program to conduct this analysis). Figure 2.4: Triangular distribution for sensitivity Positive predictive values estimated in a validation study were used to specify distributions of values for the bias parameters of sensitivity and specificity. Investigators chose triangular distributions for sensitivity and specificity (Figure 2.4). Values were repeatedly sampled at random 1,000 times from these distributions and these sampled values were used to calculate a distribution of bias-adjusted estimates. The median bias-adjusted estimate was 1.06 with 95% simulation interval 0.92-1.22. References "],["additional-examples.html", "3 Additional example applications 3.1 Selection bias 3.2 Misclassification 3.3 Unmeasured confounding 3.4 Multiple biases", " 3 Additional example applications We present here examples of quantitative bias analysis using simulated participant-level data for a cohort study with a binary treatment, binary outcome, and binary confounder. # load packages require(tidyverse) require(ggplot2) library(broom) library(janitor) 3.1 Selection bias In the simulated data we have a binary treatment \\(A\\), binary confounder \\(X\\), outcome \\(Y\\), and a binary indicator for selection into the study \\(S\\). A copy of the simulated data can be downloaded from GitHub. sim_data &lt;- read_csv(&quot;data/simulated_data.csv&quot;) %&gt;% select(x,a,y,s) sim_data %&gt;% head(5) ## # A tibble: 5 × 4 ## x a y s ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 1 ## 2 0 0 0 1 ## 3 0 0 0 1 ## 4 0 0 0 1 ## 5 0 0 0 1 The data was generated such that the causal conditional odds ratio between treatment and outcome was 2. However, in a given sample the estimate may differ due to random error. If we observed a random sample from the target population we could unbiasedly estimate the odds ratio using logistic regression. # fit logistic regression model lgr_model &lt;- glm(y ~ x + a, data=sim_data, family=&quot;binomial&quot;) # tidy model outputs or &lt;- lgr_model %&gt;% tidy(exponentiate=TRUE, conf.int=TRUE) %&gt;% filter(term == &quot;a&quot;) %&gt;% select(estimate, conf.low, conf.high) # output as table or %&gt;% knitr::kable(caption = &quot;Estimated odds ratio&quot;) Table 3.1: Estimated odds ratio estimate conf.low conf.high 2.040021 1.942229 2.142647 However, if we consider that selection into the study was dependent on exposure and outcome and that we only observed a selected subsample of the target population, then the estimated odds ratio is biased. # restrict data to selected subsample selected_data &lt;- sim_data %&gt;% filter(s == 1) # fit logistic regression model lgr_model_selected &lt;- glm(y ~ x + a, data=selected_data, family=&quot;binomial&quot;) # tidy model outputs or_selected &lt;- lgr_model_selected %&gt;% tidy(exponentiate=TRUE, conf.int=TRUE) %&gt;% filter(term == &quot;a&quot;) %&gt;% select(estimate, conf.low, conf.high) # output as table or_selected %&gt;% knitr::kable(caption = &quot;Estimated odds ratio in selected sample&quot;) Table 3.2: Estimated odds ratio in selected sample estimate conf.low conf.high 1.577843 1.494616 1.665406 3.1.1 Bias formulas One option is to apply bias formulas for the odds ratio. \\[ OR_{BiasAdjusted} = OR_{Observed}\\frac{S_{01}S_{10}}{S_{00}S_{11}} \\] Given that the data was simulated we know the selection probabilities (\\(S11=0.7\\), \\(S01=1\\), \\(S10=0.9\\), \\(S00=1\\)) and can directly plug them in to estimate a bias-adjusted odds ratio. However, in practice we will not know these probabilities and will typically specify a range of values, or for probabilistic bias analysis a distribution of values. # define bias parameters S11 &lt;- 0.7 S01 &lt;- 1 S10 &lt;- 0.9 S00 &lt;- 1 # apply bias formula bias_adjusted_or &lt;- or_selected %&gt;% mutate(across(c(estimate, conf.low, conf.high), ~ .x * (S01*S10)/(S11*S00))) # output as table bias_adjusted_or %&gt;% knitr::kable(caption = &quot;Bias-adjusted odds ratio&quot;) Table 3.3: Bias-adjusted odds ratio estimate conf.low conf.high 2.028656 1.92165 2.141236 3.1.2 Weighting Alternatively, we can weight the individual records by the inverse probability of selection and use bootstapping to calculate a confidence interval library(boot) # Add weights selected_data_with_weights &lt;- selected_data %&gt;% mutate(prob_select = a*y*S11 + (1-a)*y*S01 + a*(1-y)*S10 + (1-a)*(1-y)*S00) %&gt;% mutate(inverse_prob = 1/prob_select) # define function to estimate weighted odds ratio (needed for bootstrap function) calculate_weighted_or &lt;- function(weighted_data, i) { weighted_lgr &lt;- glm(y ~ x + a, family=&quot;binomial&quot;, data=weighted_data[i,], weights=inverse_prob) weighted_or &lt;- coef(weighted_lgr)[[&quot;a&quot;]] %&gt;% exp() return(weighted_or) } # set seed of random number generator to ensure reproducibility set.seed(747) # bootstrap calculation of confidence intervals bootstrap_estimates &lt;- boot(selected_data_with_weights, calculate_weighted_or, R=1000) # calculate bias-adjusted point estimate using entire selected subsample point_estimate &lt;- calculate_weighted_or(selected_data_with_weights, 1:nrow(selected_data_with_weights)) # calculate percentile bootstrap confidence interval conf_int &lt;- quantile(bootstrap_estimates$t, c(0.025, 0.975)) # output bias-adjusted estimate tibble(estimate=point_estimate, conf.low=conf_int[[1]], conf.high=conf_int[[2]]) %&gt;% knitr::kable() estimate conf.low conf.high 2.031339 1.920888 2.135751 If we expect selection probabilities to differ within levels of covariates then we can specify different selection probabilities for different strata of covariates. 3.2 Misclassification We will now consider some quantitative bias analysis methods for misclassification of a binary outcome. Similar approaches can be applied for misclassification of a binary exposure. With differential misclassification of the outcome, the odds ratio is biased. # load data misclassified_data &lt;- read_csv(&quot;data/simulated_data.csv&quot;) %&gt;% select(x,a,m_y,s) # fit logistic regression model with misclassified outcome lgr_model_misclassified &lt;- glm(m_y ~ x + a, data=misclassified_data, family=&quot;binomial&quot;) # tidy model outputs or_misclassified &lt;- lgr_model_misclassified %&gt;% tidy(exponentiate=TRUE, conf.int=TRUE) %&gt;% filter(term == &quot;a&quot;) %&gt;% select(estimate, conf.low, conf.high) # output as table or_misclassified %&gt;% knitr::kable(caption = &quot;Estimated odds ratio with misclassified outcome&quot;) Table 3.4: Estimated odds ratio with misclassified outcome estimate conf.low conf.high 1.737071 1.650733 1.827756 3.2.1 Bias formulas Bias formulas for misclassification typically apply to 2x2 tables or 2x2 tables stratified by covariates and require us to specify the bias parameters of sensitivity and specificity. Given that the data was simulated, we know that sensitivity and specificity among the treated were 80% and 99%, and that sensitivity and specificity among the unexposed were 100%. In practice, we do not know these values, but can estimate them using validation studies and specify a range or, for probabilistic bias analysis, distribution of plausible values. Table 3.5: Observed 2x2 table A=1 A=0 Y*=1 a b Y*=0 c d Table 3.6: Corrected 2x2 table A=1 A=0 Y=1 A B Y=0 C D # specify bias parameters sensitivity_a0 &lt;- 1 sensitivity_a1 &lt;- 0.8 specificity_a0 &lt;- 1 specificity_a1 &lt;- 0.99 # define function to correct 2x2 table correct_two_by_two &lt;- function(a, b, c, d, sensitivity_a0, sensitivity_a1, specificity_a0, specificity_a1) { A &lt;- (a - (a + c)*(1-specificity_a1))/(sensitivity_a1 + specificity_a1 - 1) B &lt;- (b - (b + d)*(1-specificity_a0))/(sensitivity_a0 + specificity_a0 - 1) C &lt;- a + c - A D &lt;- b + d - B return(c(&quot;A&quot;=A,&quot;B&quot;=B,&quot;C&quot;=C,&quot;D&quot;=D)) } # extract 2x2 table values a_x0 &lt;- misclassified_data %&gt;% filter(m_y==1, a==1, x==0) %&gt;% nrow() b_x0 &lt;- misclassified_data %&gt;% filter(m_y==1, a==0, x==0) %&gt;% nrow() c_x0 &lt;- misclassified_data %&gt;% filter(m_y==0, a==1, x==0) %&gt;% nrow() d_x0 &lt;- misclassified_data %&gt;% filter(m_y==0, a==0, x==0) %&gt;% nrow() a_x1 &lt;- misclassified_data %&gt;% filter(m_y==1, a==1, x==1) %&gt;% nrow() b_x1 &lt;- misclassified_data %&gt;% filter(m_y==1, a==0, x==1) %&gt;% nrow() c_x1 &lt;- misclassified_data %&gt;% filter(m_y==0, a==1, x==1) %&gt;% nrow() d_x1 &lt;- misclassified_data %&gt;% filter(m_y==0, a==0, x==1) %&gt;% nrow() # correct 2x2 table corr_two_by_two_x0 &lt;- correct_two_by_two(a_x0, b_x0, c_x0, d_x0, sensitivity_a0, sensitivity_a1, specificity_a0, specificity_a1) corr_two_by_two_x1 &lt;- correct_two_by_two(a_x1, b_x1, c_x1, d_x1, sensitivity_a0, sensitivity_a1, specificity_a0, specificity_a1) # extract corrected 2x2 table values A_x0 &lt;- corr_two_by_two_x0[[1]] B_x0 &lt;- corr_two_by_two_x0[[2]] C_x0 &lt;- corr_two_by_two_x0[[3]] D_x0 &lt;- corr_two_by_two_x0[[4]] A_x1 &lt;- corr_two_by_two_x1[[1]] B_x1 &lt;- corr_two_by_two_x1[[2]] C_x1 &lt;- corr_two_by_two_x1[[3]] D_x1 &lt;- corr_two_by_two_x1[[4]] # calculate Mantel-Haenszel odds ratio numerator &lt;- A_x1*D_x1/(A_x1 + B_x1 + C_x1 + D_x1) + A_x0*D_x0/(A_x0 + B_x0 + C_x0 + D_x0) denominator &lt;- B_x1*C_x1/(A_x1 + B_x1 + C_x1 + D_x1) + B_x0*C_x0/(A_x0 + B_x0 + C_x0 + D_x0) mh_estimate &lt;- numerator/denominator print(mh_estimate) ## [1] 2.003725 We can use bootstrapping to calculate a confidence interval. 3.2.2 Record-level correction For record-level correction we can as before calculate two by two tables, but as a second step use these to calculate predictive values which we can use to impute a corrected exposure. # calculate predictive values ppv_a1_x1 &lt;- sensitivity_a1*A_x1/a_x1 ppv_a0_x1 &lt;- sensitivity_a0*B_x1/b_x1 npv_a1_x1 &lt;- specificity_a1*C_x1/c_x1 npv_a0_x1 &lt;- specificity_a0*D_x1/d_x1 ppv_a1_x0 &lt;- sensitivity_a1*A_x0/a_x0 ppv_a0_x0 &lt;- sensitivity_a0*B_x0/b_x0 npv_a1_x0 &lt;- specificity_a1*C_x0/c_x0 npv_a0_x0 &lt;- specificity_a0*D_x0/d_x0 # impute outcome misclassified_data &lt;- misclassified_data %&gt;% mutate(y = rbinom(100000, 1, ppv_a1_x1*a*m_y*x + ppv_a0_x1*(1-a)*m_y*x + (1-npv_a1_x1)*a*(1-m_y)*x + (1-npv_a0_x1)*(1-a)*(1-m_y)*x + ppv_a1_x0*a*m_y*(1-x) + ppv_a0_x0*(1-a)*m_y*(1-x) + (1-npv_a1_x0)*a*(1-m_y)*(1-x) + (1-npv_a0_x0)*(1-a)*(1-m_y)*(1-x))) # fit a logistic regression model using imputed exposure glm(y ~ a + x, family=&quot;binomial&quot;, data=misclassified_data) %&gt;% tidy(exponentiate=TRUE) %&gt;% filter(term == &quot;a&quot;) %&gt;% select(estimate) ## # A tibble: 1 × 1 ## estimate ## &lt;dbl&gt; ## 1 2.01 3.2.3 Probabalistic bias analysis Rather than specify a single or range of values for the bias parameters of sensitivity and specificity we can instead specify probability distributions for these parameters and apply probabilistic bias analysis. Here for simplicity we will assume the distributions are not correlated, but we could also generate correlated bias parameters.(Fox, MacLehose, and Lash 2022) We use bootstrapping to incorporate random error. library(EnvStats) # draw bias parameters from triangular distributions sensitivity_a1 &lt;- rtri(1000, min=0.75, mode=0.8, max=0.85) specificity_a1 &lt;- rtri(1000, min=0.985, mode=0.99, max=0.995) # specify fixed bias parameters sensitivity_a0 &lt;- 1 specificity_a0 &lt;- 1 # set seed for random number generator (for reproducibility) set.seed(2041) estimates &lt;- NULL for (i in 1:1000) { bootstrap_indices &lt;- sample(1:nrow(misclassified_data), nrow(misclassified_data), replace=TRUE) # extract 2x2 table values a_x0 &lt;- misclassified_data[bootstrap_indices,] %&gt;% filter(m_y==1, a==1, x==0) %&gt;% nrow() b_x0 &lt;- misclassified_data[bootstrap_indices,] %&gt;% filter(m_y==1, a==0, x==0) %&gt;% nrow() c_x0 &lt;- misclassified_data[bootstrap_indices,] %&gt;% filter(m_y==0, a==1, x==0) %&gt;% nrow() d_x0 &lt;- misclassified_data[bootstrap_indices,] %&gt;% filter(m_y==0, a==0, x==0) %&gt;% nrow() a_x1 &lt;- misclassified_data[bootstrap_indices,] %&gt;% filter(m_y==1, a==1, x==1) %&gt;% nrow() b_x1 &lt;- misclassified_data[bootstrap_indices,] %&gt;% filter(m_y==1, a==0, x==1) %&gt;% nrow() c_x1 &lt;- misclassified_data[bootstrap_indices,] %&gt;% filter(m_y==0, a==1, x==1) %&gt;% nrow() d_x1 &lt;- misclassified_data[bootstrap_indices,] %&gt;% filter(m_y==0, a==0, x==1) %&gt;% nrow() # calculate corrected 2x2 table corr_two_by_two_x0 &lt;- correct_two_by_two(a_x0, b_x0, c_x0, d_x0, sensitivity_a0, sensitivity_a1[i], specificity_a0, specificity_a1[i]) corr_two_by_two_x1 &lt;- correct_two_by_two(a_x1, b_x1, c_x1, d_x1, sensitivity_a0, sensitivity_a1[i], specificity_a0, specificity_a1[i]) # extract corrected 2x2 table values A_x0 &lt;- corr_two_by_two_x0[[1]] B_x0 &lt;- corr_two_by_two_x0[[2]] C_x0 &lt;- corr_two_by_two_x0[[3]] D_x0 &lt;- corr_two_by_two_x0[[4]] A_x1 &lt;- corr_two_by_two_x1[[1]] B_x1 &lt;- corr_two_by_two_x1[[2]] C_x1 &lt;- corr_two_by_two_x1[[3]] D_x1 &lt;- corr_two_by_two_x1[[4]] # calculate Mantel-Haenszel odds ratio numerator &lt;- A_x1*D_x1/(A_x1 + B_x1 + C_x1 + D_x1) + A_x0*D_x0/(A_x0 + B_x0 + C_x0 + D_x0) denominator &lt;- B_x1*C_x1/(A_x1 + B_x1 + C_x1 + D_x1) + B_x0*C_x0/(A_x0 + B_x0 + C_x0 + D_x0) estimates &lt;- bind_rows(estimates, tibble_row(bias_adj_or = numerator/denominator)) } # calculate median and 95% simulation interval of bias-adjusted estimates quantile(estimates$bias_adj_or, c(0.025, 0.5, 0.975)) ## 2.5% 50% 97.5% ## 1.830828 2.009066 2.206712 # plot distribution of bias-adjusted estimates ggplot(data=estimates) + geom_density(aes(x=bias_adj_or), fill=&quot;darkblue&quot;, alpha=0.5) + xlab(&quot;Bias-adjusted odds ratio&quot;) Figure 3.1: Distribution of bias-adjusted estimates 3.3 Unmeasured confounding If the confounder, \\(X\\), had not been measured then we would not be able to adjust for it, and our estimates would be biased. # restrict data to selected subsample unmeasured_confounder_data &lt;- sim_data %&gt;% select(a,y) # fit logistic regression model without confounder lgr_model_confounded &lt;- glm(y ~ a, family=&quot;binomial&quot;, data=unmeasured_confounder_data) # tidy model outputs or_confounded &lt;- lgr_model_confounded %&gt;% tidy(exponentiate=TRUE, conf.int=TRUE) %&gt;% filter(term == &quot;a&quot;) %&gt;% select(estimate, conf.low, conf.high) # output as table or_confounded %&gt;% knitr::kable(caption = &quot;Estimated odds ratio with unmeasured confounder&quot;) Table 3.7: Estimated odds ratio with unmeasured confounder estimate conf.low conf.high 2.321895 2.213997 2.434989 3.3.1 Bias formula Given that the outcome is rare we can apply a bias formula for the risk ratio. \\[ RR_{ZY}^{BiasAdj} = RR_{ZY}^{Obs}\\frac{1 + P(U=1|Z=0)(RR_{UY|Z}-1)}{1 + P(U=1|Z=1)(RR_{UY|Z}-1)} \\] # define bias parameter PU1 &lt;- 0.33 PU0 &lt;- 0.14 RR_UY &lt;- 2 # define function to calculate bias-adjusted risk ratio uconf_bias_adj_rr &lt;- function(rr_zy, pu1, pu0, rr_uy) { bias_adj_rr &lt;- rr_zy * (1 + pu0*(rr_uy - 1))/(1 + pu1*(rr_uy - 1)) return(bias_adj_rr) } # calculate bias-adjusted odds ratios or_confounded %&gt;% mutate(across(c(estimate, conf.low, conf.high), ~ uconf_bias_adj_rr(.x, 0.33, 0.14, 2))) %&gt;% knitr::kable(caption = &quot;Bias-adjusted odds ratio for unmeasured confounding&quot;) Table 3.8: Bias-adjusted odds ratio for unmeasured confounding estimate conf.low conf.high 1.990196 1.897711 2.087134 3.4 Multiple biases Finally, we consider a setting with both selection bias, misclassification, and unmeasured confounding.First, we correct for misclassification and selection to estimate the crude odds ratio, then we apply a bias formula to adjust for confounding bias. multiple_bias_data &lt;- read_csv(&quot;data/simulated_data.csv&quot;) %&gt;% filter(s==1) %&gt;% select(a,m_y) # specify bias parameters sensitivity_a0 &lt;- 1 sensitivity_a1 &lt;- 0.8 specificity_a0 &lt;- 1 specificity_a1 &lt;- 0.99 ## record-level correction for misclassification # extract 2x2 table values a &lt;- multiple_bias_data %&gt;% filter(m_y==1, a==1) %&gt;% nrow() b &lt;- multiple_bias_data %&gt;% filter(m_y==1, a==0) %&gt;% nrow() c &lt;- multiple_bias_data %&gt;% filter(m_y==0, a==1) %&gt;% nrow() d &lt;- multiple_bias_data %&gt;% filter(m_y==0, a==0) %&gt;% nrow() # correct 2x2 table corr_two_by_two &lt;- correct_two_by_two(a, b, c, d, sensitivity_a0, sensitivity_a1, specificity_a0, specificity_a1) # extract corrected 2x2 table values A &lt;- corr_two_by_two[[1]] B &lt;- corr_two_by_two[[2]] C &lt;- corr_two_by_two[[3]] D &lt;- corr_two_by_two[[4]] # calculate predictive values ppv_a1 &lt;- sensitivity_a1*A/a ppv_a0 &lt;- sensitivity_a0*B/b npv_a1 &lt;- specificity_a1*C/c npv_a0 &lt;- specificity_a0*D/d # impute outcome multiple_bias_data &lt;- multiple_bias_data %&gt;% mutate(y = rbinom(nrow(multiple_bias_data), 1, ppv_a1*a*m_y + ppv_a0*(1-a)*m_y + (1-npv_a1)*a*(1-m_y) + (1-npv_a0)*(1-a)*(1-m_y))) # calculate crude odds-ratio multiple_bias_lgr &lt;- glm(y ~ a, family=&quot;binomial&quot;, data=multiple_bias_data) or_adj_misc &lt;- coef(multiple_bias_lgr)[&quot;a&quot;] %&gt;% exp() # correct odds ratio for selection bias or_adj_sel &lt;- or_adj_misc * (S01*S10)/(S11*S00) # apply bias formula for confounding uconf_bias_adj_rr(or_adj_sel, 0.33, 0.14, 2) ## a ## 1.942437 References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
